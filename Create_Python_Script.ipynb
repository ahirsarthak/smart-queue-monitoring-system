{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Create the Python Script\n",
    "\n",
    "In the cell below, you will need to complete the Python script and run the cell to generate the file using the magic `%%writefile` command. Your main task is to complete the following methods for the `PersonDetect` class:\n",
    "* `load_model`\n",
    "* `predict`\n",
    "* `draw_outputs`\n",
    "* `preprocess_outputs`\n",
    "* `preprocess_inputs`\n",
    "\n",
    "For your reference, here are all the arguments used for the argument parser in the command line:\n",
    "* `--model`:  The file path of the pre-trained IR model, which has been pre-processed using the model optimizer. There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware.\n",
    "* `--device`: The type of hardware you want to load the model on (CPU, GPU, MYRIAD, HETERO:FPGA,CPU)\n",
    "* `--video`: The file path of the input video.\n",
    "* `--output_path`: The location where the output stats and video file with inference needs to be stored (results/[device]).\n",
    "* `--max_people`: The max number of people in queue before directing a person to another queue.\n",
    "* `--threshold`: The probability threshold value for the person detection. Optional arg; default value is 0.60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting black\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/bb/ad34bbc93d1bea3de086d7c59e528d4a503ac8fe318bd1fa48605584c3d2/black-19.10b0-py36-none-any.whl (97kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 3.6MB/s a 0:00:011\n",
      "\u001b[?25hCollecting flake8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/20/6326a9a0c6f0527612bae748c4c03df5cd69cf06dfb2cf59d85c6e165a6a/flake8-3.8.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 8.5MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting nb_black\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/86/195e7b1bcafe86c2f5deb3efcbdc6ab35f7269fc4b0095b94321de2215a2/nb_black-1.0.7.tar.gz\n",
      "Collecting ipython\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/e8/47fda10c3ab103d9d4a667b40da9afd542c4e50aeb00c861b4eee5bb4e8f/ipython-7.15.0-py3-none-any.whl (783kB)\n",
      "\u001b[K    100% |████████████████████████████████| 788kB 6.7MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pathspec<1,>=0.6 (from black)\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/d0/887c58853bd4b6ffc7aa9cdba4fc57d7b979b45888a6bd47e4568e1cf868/pathspec-0.8.0-py2.py3-none-any.whl\n",
      "Collecting regex (from black)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/86/461fa59347051f378e661fcaf0a6d793450c779f4a29f338ae37c52d8575/regex-2020.6.8-cp36-cp36m-manylinux1_x86_64.whl (660kB)\n",
      "\u001b[K    100% |████████████████████████████████| 665kB 10.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: click>=6.5 in /opt/conda/lib/python3.6/site-packages (from black) (6.7)\n",
      "Collecting appdirs (from black)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
      "Collecting typed-ast>=1.4.0 (from black)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
      "\u001b[K    100% |████████████████████████████████| 747kB 12.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs>=18.1.0 in /opt/conda/lib/python3.6/site-packages (from black) (19.1.0)\n",
      "Collecting toml>=0.9.4 (from black)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/e1/1b40b80f2e1663a6b9f497123c11d7d988c0919abbf3c3f2688e448c5363/toml-0.10.1-py2.py3-none-any.whl\n",
      "Collecting mccabe<0.7.0,>=0.6.0 (from flake8)\n",
      "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
      "Collecting pycodestyle<2.7.0,>=2.6.0a1 (from flake8)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 16.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\" (from flake8)\n",
      "  Downloading https://files.pythonhosted.org/packages/98/13/a1d703ec396ade42c1d33df0e1cb691a28b7c08b336a5683912c87e04cd7/importlib_metadata-1.6.1-py2.py3-none-any.whl\n",
      "Collecting pyflakes<2.3.0,>=2.2.0 (from flake8)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/5b/fd01b0c696f2f9a6d2c839883b642493b431f28fa32b29abc465ef675473/pyflakes-2.2.0-py2.py3-none-any.whl (66kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 18.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: traitlets>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipython) (4.3.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython) (38.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /opt/conda/lib/python3.6/site-packages (from ipython) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: backcall in /opt/conda/lib/python3.6/site-packages (from ipython) (0.1.0)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 (from ipython)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/a7/81b39aa50e9284fe2cb21cc7fb7de7817b224172d42793fd57451d38842b/prompt_toolkit-3.0.5-py3-none-any.whl (351kB)\n",
      "\u001b[K    100% |████████████████████████████████| 358kB 22.9MB/s ta 0:00:01    61% |███████████████████▋            | 215kB 17.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython) (4.3.1)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /opt/conda/lib/python3.6/site-packages (from ipython) (4.0.11)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython) (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython) (0.10.2)\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->flake8)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2->ipython) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2->ipython) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython) (0.5.2)\n",
      "Building wheels for collected packages: nb-black\n",
      "  Running setup.py bdist_wheel for nb-black ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/35/6d/99/856a3401e3a2153f51223f1be990e4b30f80f9e4bd1604ae9b\n",
      "Successfully built nb-black\n",
      "\u001b[31mipywidgets 7.0.5 has requirement widgetsnbextension~=3.0.0, but you'll have widgetsnbextension 3.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pathspec, regex, appdirs, typed-ast, toml, black, mccabe, pycodestyle, zipp, importlib-metadata, pyflakes, flake8, prompt-toolkit, ipython, nb-black\n",
      "  Found existing installation: prompt-toolkit 1.0.15\n",
      "    Uninstalling prompt-toolkit-1.0.15:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.15\n",
      "  Found existing installation: ipython 6.5.0\n",
      "    Uninstalling ipython-6.5.0:\n",
      "      Successfully uninstalled ipython-6.5.0\n",
      "Successfully installed appdirs-1.4.4 black-19.10b0 flake8-3.8.3 importlib-metadata-1.6.1 ipython-7.15.0 mccabe-0.6.1 nb-black-1.0.7 pathspec-0.8.0 prompt-toolkit-3.0.5 pycodestyle-2.6.0 pyflakes-2.2.0 regex-2020.6.8 toml-0.10.1 typed-ast-1.4.1 zipp-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U black flake8 nb_black ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting person_detect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile person_detect.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import subprocess\n",
    "import logging\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except BaseException:\n",
    "    tqdm = None\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Queue:\n",
    "    \"\"\"Class for dealing with queues.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.queues = []\n",
    "\n",
    "    def add_queue(self, points):\n",
    "        self.queues.append(points)\n",
    "\n",
    "    def get_queues(self, image):\n",
    "        for q in self.queues:\n",
    "            x_min, y_min, x_max, y_max = q\n",
    "            frame = image[y_min:y_max, x_min:x_max]\n",
    "            yield frame\n",
    "\n",
    "    def check_coords(self, coords):\n",
    "        d = {k + 1: 0 for k in range(len(self.queues))}\n",
    "        for coord in coords:\n",
    "            for i, q in enumerate(self.queues):\n",
    "                if coord[0] > q[0] and coord[2] < q[2]:\n",
    "                    d[i + 1] += 1\n",
    "        return d\n",
    "\n",
    "\n",
    "class PersonDetect:\n",
    "    \"\"\"Class for the Person Detection Model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name, device, threshold=0.60):\n",
    "        self.model_weights = model_name + \".bin\"\n",
    "        self.model_structure = model_name + \".xml\"\n",
    "        assert os.path.isfile(self.model_structure) and os.path.isfile(\n",
    "            self.model_weights\n",
    "        )\n",
    "        self.device = device\n",
    "        self.threshold = threshold\n",
    "        self._model_size = os.stat(self.model_weights).st_size / 1024.0 ** 2\n",
    "\n",
    "        self._ie_core = IECore()\n",
    "        self.model = self._get_model()\n",
    "\n",
    "        # Get the input layer\n",
    "        self.input_name = next(iter(self.model.inputs))\n",
    "        self.input_shape = self.model.inputs[self.input_name].shape\n",
    "        self.output_name = next(iter(self.model.outputs))\n",
    "        self.output_shape = self.model.outputs[self.output_name].shape\n",
    "        self._init_image_w = None\n",
    "        self._init_image_h = None\n",
    "\n",
    "    def _get_model(self):\n",
    "        \"\"\"Helper function for reading the network.\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                model = self._ie_core.read_network(\n",
    "                    model=self.model_structure, weights=self.model_weights\n",
    "                )\n",
    "            except AttributeError:\n",
    "                model = IENetwork(\n",
    "                    model=self.model_structure, weights=self.model_weights\n",
    "                )\n",
    "        except Exception:\n",
    "            raise ValueError(\n",
    "                \"Could not Initialise the network. \"\n",
    "                \"Have you entered the correct model path?\"\n",
    "            )\n",
    "        else:\n",
    "            return model\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the model.\"\"\"\n",
    "        # Load the model into the plugin\n",
    "        self.exec_network = self._ie_core.load_network(\n",
    "            network=self.model, device_name=self.device\n",
    "        )\n",
    "\n",
    "    def predict(self, image, request_id=0):\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            raise IOError(\"Image not parsed correctly.\")\n",
    "\n",
    "        p_image = self.preprocess_input(image)\n",
    "        self.exec_network.start_async(\n",
    "            request_id=request_id, inputs={self.input_name: p_image}\n",
    "        )\n",
    "        status = self.exec_network.requests[request_id].wait(-1)\n",
    "        if status == 0:\n",
    "            result = self.exec_network.requests[request_id].outputs[self.output_name]\n",
    "            return self.draw_outputs(result, image)\n",
    "\n",
    "    def draw_outputs(self, inference_blob, image):\n",
    "        \"\"\"Draw bounding boxes onto the frame.\"\"\"\n",
    "        if not (self._init_image_w and self._init_image_h):\n",
    "            raise RuntimeError(\"Initial image width and height cannot be None.\")\n",
    "        label = \"Person\"\n",
    "        bbox_color = (0, 255, 0)\n",
    "        padding_size = (0.05, 0.25)\n",
    "        text_color = (255, 255, 255)\n",
    "        text_scale = 1.5\n",
    "        text_thickness = 1\n",
    "\n",
    "        coords = []\n",
    "        for box in inference_blob[0][0]:  # Output shape is 1x1xNx7\n",
    "            conf = box[2]\n",
    "            if conf >= self.threshold:\n",
    "                xmin = int(box[3] * self._init_image_w)\n",
    "                ymin = int(box[4] * self._init_image_h)\n",
    "                xmax = int(box[5] * self._init_image_w)\n",
    "                ymax = int(box[6] * self._init_image_h)\n",
    "                coords.append((xmin, ymin, xmax, ymax))\n",
    "\n",
    "                cv2.rectangle(\n",
    "                    image, (xmin, ymin), (xmax, ymax,), color=bbox_color, thickness=2,\n",
    "                )\n",
    "\n",
    "                ((label_width, label_height), _) = cv2.getTextSize(\n",
    "                    label,\n",
    "                    cv2.FONT_HERSHEY_PLAIN,\n",
    "                    fontScale=text_scale,\n",
    "                    thickness=text_thickness,\n",
    "                )\n",
    "\n",
    "                cv2.rectangle(\n",
    "                    image,\n",
    "                    (xmin, ymin),\n",
    "                    (\n",
    "                        int(xmin + label_width + label_width * padding_size[0]),\n",
    "                        int(ymin + label_height + label_height * padding_size[1]),\n",
    "                    ),\n",
    "                    color=bbox_color,\n",
    "                    thickness=cv2.FILLED,\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    label,\n",
    "                    org=(\n",
    "                        xmin,\n",
    "                        int(ymin + label_height + label_height * padding_size[1]),\n",
    "                    ),\n",
    "                    fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "                    fontScale=text_scale,\n",
    "                    color=text_color,\n",
    "                    thickness=text_thickness,\n",
    "                )\n",
    "\n",
    "        return coords, image\n",
    "\n",
    "    def preprocess_input(self, image):\n",
    "        \"\"\"Helper function for processing frame\"\"\"\n",
    "        p_frame = cv2.resize(image, (self.input_shape[3], self.input_shape[2]))\n",
    "        # Change data layout from HWC to CHW\n",
    "        p_frame = p_frame.transpose((2, 0, 1))\n",
    "        p_frame = p_frame.reshape(1, *p_frame.shape)\n",
    "        return p_frame\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    start_model_load_time = time.time()\n",
    "    pd = PersonDetect(args.model, args.device, args.threshold)\n",
    "    pd.load_model()\n",
    "    total_model_load_time = time.time() - start_model_load_time\n",
    "\n",
    "    queue = Queue()\n",
    "\n",
    "    try:\n",
    "        queue_param = np.load(args.queue_param)\n",
    "        filename = os.path.split(args.video)[-1].split(\".\")[0] + \".npy\"\n",
    "        np.save(os.path.join(args.output_path, filename), queue_param)\n",
    "        for q in queue_param:\n",
    "            queue.add_queue(q)\n",
    "    except Exception:\n",
    "        logger.exception(\"Error loading queue param file\")\n",
    "\n",
    "    try:\n",
    "        assert os.path.isfile(args.video)\n",
    "        cap = cv2.VideoCapture(args.video)\n",
    "    except (FileNotFoundError, TypeError, AssertionError):\n",
    "        logger.exception(f\"Cannot locate video file: {args.video}\")\n",
    "        raise\n",
    "    except Exception as err:\n",
    "        logger.exception(f\"Something else went wrong with the video file: {err}\")\n",
    "        raise\n",
    "\n",
    "    pd._init_image_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    pd._init_image_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    if tqdm:\n",
    "        pbar = tqdm(total=int(video_len - fps + 1))\n",
    "\n",
    "    out_video = cv2.VideoWriter(\n",
    "        os.path.join(args.output_path, \"output_video.mp4\"),\n",
    "        cv2.VideoWriter_fourcc(*\"avc1\"),\n",
    "        fps,\n",
    "        (pd._init_image_w, pd._init_image_h),\n",
    "        True,\n",
    "    )\n",
    "\n",
    "    counter = 0\n",
    "    start_inference_time = time.time()\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            counter += 1\n",
    "\n",
    "            if tqdm:\n",
    "                pbar.update(1)\n",
    "\n",
    "            predict_start_time = time.time()\n",
    "            coords, image = pd.predict(frame)\n",
    "            total_inference_time_taken = time.time() - predict_start_time\n",
    "            message = f\"Inference time: {total_inference_time_taken*1000:.2f}ms\"\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                message,\n",
    "                (15, pd._init_image_h - 50),\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                0.75,\n",
    "                (255, 255, 255),\n",
    "                1,\n",
    "            )\n",
    "            num_people = queue.check_coords(coords)\n",
    "\n",
    "            if tqdm:\n",
    "                tqdm.write(f\"Total People in frame = {len(coords)}\")\n",
    "                tqdm.write(f\"Number of people in queue = {num_people}\")\n",
    "            else:\n",
    "                print(f\"Total People in frame = {len(coords)}\")\n",
    "                print(f\"Number of people in queue = {num_people}\")\n",
    "\n",
    "            out_text = \"\"\n",
    "            y_pixel = 25\n",
    "\n",
    "            for k, v in num_people.items():\n",
    "                out_text += f\"No. of People in Queue {k} is {v} \"\n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    out_text,\n",
    "                    (15, y_pixel),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    1,\n",
    "                    (0, 255, 0),\n",
    "                    2,\n",
    "                )\n",
    "                if v >= int(args.max_people):\n",
    "                    out_text += \" Queue full; Please move to next Queue!\"\n",
    "                    cv2.putText(\n",
    "                        image,\n",
    "                        out_text,\n",
    "                        (15, y_pixel),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        1,\n",
    "                        (0, 0, 255),\n",
    "                        2,\n",
    "                    )\n",
    "                out_text = \"\"\n",
    "                y_pixel += 40\n",
    "\n",
    "            # print total_inference_time_taken\n",
    "            if args.debug:\n",
    "                cv2.imshow(\"Frame\", image)\n",
    "            else:\n",
    "                out_video.write(image)\n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            # if the `q` key was pressed, break from the loop\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        total_time = time.time() - start_inference_time\n",
    "        total_inference_time = round(total_time, 1)\n",
    "        fps = counter / total_inference_time\n",
    "        print(f\"Total time it took to run Inference: {total_inference_time}s\")\n",
    "        print(f\"Frames/Second: {fps}\")\n",
    "\n",
    "        with open(os.path.join(args.output_path, \"stats.txt\"), \"w\") as f:\n",
    "            f.write(str(total_inference_time) + \"\\n\")\n",
    "            f.write(str(fps) + \"\\n\")\n",
    "            f.write(str(total_model_load_time) + \"\\n\")\n",
    "\n",
    "        if tqdm:\n",
    "            pbar.close()\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Could not run Inference: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        required=True,\n",
    "        help=(\n",
    "            \"The file path of the pre-trained IR model, which has been pre-processed \"\n",
    "            \"using the model optimizer. There is automated support built in this \"\n",
    "            \"argument to support both FP32 and FP16 models targeting different hardware.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        default=\"CPU\",\n",
    "        help=(\n",
    "            \"The type of hardware you want to load the model on \"\n",
    "            \"(CPU, GPU, MYRIAD, HETERO:FPGA,CPU): [default: CPU]\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--video\", default=None, help=\"The file path of the input video.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_path\",\n",
    "        default=\"/results\",\n",
    "        help=(\n",
    "            \"The location where the output stats and video file with inference needs \"\n",
    "            \"to be stored (results/[device]).\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_people\",\n",
    "        default=2,\n",
    "        help=(\n",
    "            \"The max number of people in queue before directing a person to \"\n",
    "            \"another queue.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--threshold\",\n",
    "        default=0.60,\n",
    "        help=(\n",
    "            \"The probability threshold value for the person detection. \"\n",
    "            \"Optional arg; default value is 0.60.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--queue_param\", default=None)\n",
    "    parser.add_argument(\n",
    "        \"--debug\", action=\"store_true\", help=\"Show output on screen [debugging].\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "Now that you've run the above cell and created your Python script, you will create your job submission shell script in the next workspace.\n",
    "\n",
    "**Note**: As a reminder, if you need to make any changes to the Python script, you can come back to this workspace to edit and run the above cell to overwrite the file with your changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mreformatted person_detect.py\u001b[0m\n",
      "\u001b[1mAll done! ✨ 🍰 ✨\u001b[0m\n",
      "\u001b[1m1 file reformatted\u001b[0m.\u001b[0m\n",
      "person_detect.py:5:1: F401 'sys' imported but unused\n",
      "person_detect.py:7:1: F401 'subprocess' imported but unused\n"
     ]
    }
   ],
   "source": [
    "!black -l 90 person_detect.py\n",
    "!flake8 --max-line-length=90 person_detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__pycache__/person_detect.cpython-36.pyc'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py_compile\n",
    "\n",
    "py_compile.compile(\"person_detect.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t#!/usr/bin/env python3\r\n",
      "     2\t\r\n",
      "     3\timport argparse\r\n",
      "     4\timport os\r\n",
      "     5\timport sys\r\n",
      "     6\timport time\r\n",
      "     7\timport subprocess\r\n",
      "     8\timport logging\r\n",
      "     9\t\r\n",
      "    10\timport cv2\r\n",
      "    11\timport numpy as np\r\n",
      "    12\t\r\n",
      "    13\tfrom openvino.inference_engine import IENetwork, IECore\r\n",
      "    14\t\r\n",
      "    15\ttry:\r\n",
      "    16\t    from tqdm import tqdm\r\n",
      "    17\texcept BaseException:\r\n",
      "    18\t    tqdm = None\r\n",
      "    19\t\r\n",
      "    20\tlogger = logging.getLogger(__name__)\r\n",
      "    21\t\r\n",
      "    22\t\r\n",
      "    23\tclass Queue:\r\n",
      "    24\t    \"\"\"Class for dealing with queues.\"\"\"\r\n",
      "    25\t\r\n",
      "    26\t    def __init__(self):\r\n",
      "    27\t        self.queues = []\r\n",
      "    28\t\r\n",
      "    29\t    def add_queue(self, points):\r\n",
      "    30\t        self.queues.append(points)\r\n",
      "    31\t\r\n",
      "    32\t    def get_queues(self, image):\r\n",
      "    33\t        for q in self.queues:\r\n",
      "    34\t            x_min, y_min, x_max, y_max = q\r\n",
      "    35\t            frame = image[y_min:y_max, x_min:x_max]\r\n",
      "    36\t            yield frame\r\n",
      "    37\t\r\n",
      "    38\t    def check_coords(self, coords):\r\n",
      "    39\t        d = {k + 1: 0 for k in range(len(self.queues))}\r\n",
      "    40\t        for coord in coords:\r\n",
      "    41\t            for i, q in enumerate(self.queues):\r\n",
      "    42\t                if coord[0] > q[0] and coord[2] < q[2]:\r\n",
      "    43\t                    d[i + 1] += 1\r\n",
      "    44\t        return d\r\n",
      "    45\t\r\n",
      "    46\t\r\n",
      "    47\tclass PersonDetect:\r\n",
      "    48\t    \"\"\"Class for the Person Detection Model.\"\"\"\r\n",
      "    49\t\r\n",
      "    50\t    def __init__(self, model_name, device, threshold=0.60):\r\n",
      "    51\t        self.model_weights = model_name + \".bin\"\r\n",
      "    52\t        self.model_structure = model_name + \".xml\"\r\n",
      "    53\t        assert os.path.isfile(self.model_structure) and os.path.isfile(self.model_weights)\r\n",
      "    54\t        self.device = device\r\n",
      "    55\t        self.threshold = threshold\r\n",
      "    56\t        self._model_size = os.stat(self.model_weights).st_size / 1024.0 ** 2\r\n",
      "    57\t\r\n",
      "    58\t        self._ie_core = IECore()\r\n",
      "    59\t        self.model = self._get_model()\r\n",
      "    60\t\r\n",
      "    61\t        # Get the input layer\r\n",
      "    62\t        self.input_name = next(iter(self.model.inputs))\r\n",
      "    63\t        self.input_shape = self.model.inputs[self.input_name].shape\r\n",
      "    64\t        self.output_name = next(iter(self.model.outputs))\r\n",
      "    65\t        self.output_shape = self.model.outputs[self.output_name].shape\r\n",
      "    66\t        self._init_image_w = None\r\n",
      "    67\t        self._init_image_h = None\r\n",
      "    68\t\r\n",
      "    69\t    def _get_model(self):\r\n",
      "    70\t        \"\"\"Helper function for reading the network.\"\"\"\r\n",
      "    71\t        try:\r\n",
      "    72\t            try:\r\n",
      "    73\t                model = self._ie_core.read_network(\r\n",
      "    74\t                    model=self.model_structure, weights=self.model_weights\r\n",
      "    75\t                )\r\n",
      "    76\t            except AttributeError:\r\n",
      "    77\t                model = IENetwork(model=self.model_structure, weights=self.model_weights)\r\n",
      "    78\t        except Exception:\r\n",
      "    79\t            raise ValueError(\r\n",
      "    80\t                \"Could not Initialise the network. \"\r\n",
      "    81\t                \"Have you entered the correct model path?\"\r\n",
      "    82\t            )\r\n",
      "    83\t        else:\r\n",
      "    84\t            return model\r\n",
      "    85\t\r\n",
      "    86\t    def load_model(self):\r\n",
      "    87\t        \"\"\"Load the model.\"\"\"\r\n",
      "    88\t        # Load the model into the plugin\r\n",
      "    89\t        self.exec_network = self._ie_core.load_network(\r\n",
      "    90\t            network=self.model, device_name=self.device\r\n",
      "    91\t        )\r\n",
      "    92\t\r\n",
      "    93\t    def predict(self, image, request_id=0):\r\n",
      "    94\t        if not isinstance(image, np.ndarray):\r\n",
      "    95\t            raise IOError(\"Image not parsed correctly.\")\r\n",
      "    96\t\r\n",
      "    97\t        p_image = self.preprocess_input(image)\r\n",
      "    98\t        self.exec_network.start_async(\r\n",
      "    99\t            request_id=request_id, inputs={self.input_name: p_image}\r\n",
      "   100\t        )\r\n",
      "   101\t        status = self.exec_network.requests[request_id].wait(-1)\r\n",
      "   102\t        if status == 0:\r\n",
      "   103\t            result = self.exec_network.requests[request_id].outputs[self.output_name]\r\n",
      "   104\t            return self.draw_outputs(result, image)\r\n",
      "   105\t\r\n",
      "   106\t    def draw_outputs(self, inference_blob, image):\r\n",
      "   107\t        \"\"\"Draw bounding boxes onto the frame.\"\"\"\r\n",
      "   108\t        if not (self._init_image_w and self._init_image_h):\r\n",
      "   109\t            raise RuntimeError(\"Initial image width and height cannot be None.\")\r\n",
      "   110\t        label = \"Person\"\r\n",
      "   111\t        bbox_color = (0, 255, 0)\r\n",
      "   112\t        padding_size = (0.05, 0.25)\r\n",
      "   113\t        text_color = (255, 255, 255)\r\n",
      "   114\t        text_scale = 1.5\r\n",
      "   115\t        text_thickness = 1\r\n",
      "   116\t\r\n",
      "   117\t        coords = []\r\n",
      "   118\t        for box in inference_blob[0][0]:  # Output shape is 1x1xNx7\r\n",
      "   119\t            conf = box[2]\r\n",
      "   120\t            if conf >= self.threshold:\r\n",
      "   121\t                xmin = int(box[3] * self._init_image_w)\r\n",
      "   122\t                ymin = int(box[4] * self._init_image_h)\r\n",
      "   123\t                xmax = int(box[5] * self._init_image_w)\r\n",
      "   124\t                ymax = int(box[6] * self._init_image_h)\r\n",
      "   125\t                coords.append((xmin, ymin, xmax, ymax))\r\n",
      "   126\t\r\n",
      "   127\t                cv2.rectangle(\r\n",
      "   128\t                    image, (xmin, ymin), (xmax, ymax,), color=bbox_color, thickness=2,\r\n",
      "   129\t                )\r\n",
      "   130\t\r\n",
      "   131\t                ((label_width, label_height), _) = cv2.getTextSize(\r\n",
      "   132\t                    label,\r\n",
      "   133\t                    cv2.FONT_HERSHEY_PLAIN,\r\n",
      "   134\t                    fontScale=text_scale,\r\n",
      "   135\t                    thickness=text_thickness,\r\n",
      "   136\t                )\r\n",
      "   137\t\r\n",
      "   138\t                cv2.rectangle(\r\n",
      "   139\t                    image,\r\n",
      "   140\t                    (xmin, ymin),\r\n",
      "   141\t                    (\r\n",
      "   142\t                        int(xmin + label_width + label_width * padding_size[0]),\r\n",
      "   143\t                        int(ymin + label_height + label_height * padding_size[1]),\r\n",
      "   144\t                    ),\r\n",
      "   145\t                    color=bbox_color,\r\n",
      "   146\t                    thickness=cv2.FILLED,\r\n",
      "   147\t                )\r\n",
      "   148\t                cv2.putText(\r\n",
      "   149\t                    image,\r\n",
      "   150\t                    label,\r\n",
      "   151\t                    org=(\r\n",
      "   152\t                        xmin,\r\n",
      "   153\t                        int(ymin + label_height + label_height * padding_size[1]),\r\n",
      "   154\t                    ),\r\n",
      "   155\t                    fontFace=cv2.FONT_HERSHEY_PLAIN,\r\n",
      "   156\t                    fontScale=text_scale,\r\n",
      "   157\t                    color=text_color,\r\n",
      "   158\t                    thickness=text_thickness,\r\n",
      "   159\t                )\r\n",
      "   160\t\r\n",
      "   161\t        return coords, image\r\n",
      "   162\t\r\n",
      "   163\t    def preprocess_input(self, image):\r\n",
      "   164\t        \"\"\"Helper function for processing frame\"\"\"\r\n",
      "   165\t        p_frame = cv2.resize(image, (self.input_shape[3], self.input_shape[2]))\r\n",
      "   166\t        # Change data layout from HWC to CHW\r\n",
      "   167\t        p_frame = p_frame.transpose((2, 0, 1))\r\n",
      "   168\t        p_frame = p_frame.reshape(1, *p_frame.shape)\r\n",
      "   169\t        return p_frame\r\n",
      "   170\t\r\n",
      "   171\t\r\n",
      "   172\tdef main(args):\r\n",
      "   173\t\r\n",
      "   174\t    start_model_load_time = time.time()\r\n",
      "   175\t    pd = PersonDetect(args.model, args.device, args.threshold)\r\n",
      "   176\t    pd.load_model()\r\n",
      "   177\t    total_model_load_time = time.time() - start_model_load_time\r\n",
      "   178\t\r\n",
      "   179\t    queue = Queue()\r\n",
      "   180\t\r\n",
      "   181\t    try:\r\n",
      "   182\t        queue_param = np.load(args.queue_param)\r\n",
      "   183\t        filename = os.path.split(args.video)[-1].split(\".\")[0] + \".npy\"\r\n",
      "   184\t        np.save(os.path.join(args.output_path, filename), queue_param)\r\n",
      "   185\t        for q in queue_param:\r\n",
      "   186\t            queue.add_queue(q)\r\n",
      "   187\t    except Exception:\r\n",
      "   188\t        logger.exception(\"Error loading queue param file\")\r\n",
      "   189\t\r\n",
      "   190\t    try:\r\n",
      "   191\t        assert os.path.isfile(args.video)\r\n",
      "   192\t        cap = cv2.VideoCapture(args.video)\r\n",
      "   193\t    except (FileNotFoundError, TypeError, AssertionError):\r\n",
      "   194\t        logger.exception(f\"Cannot locate video file: {args.video}\")\r\n",
      "   195\t        raise\r\n",
      "   196\t    except Exception as err:\r\n",
      "   197\t        logger.exception(f\"Something else went wrong with the video file: {err}\")\r\n",
      "   198\t        raise\r\n",
      "   199\t\r\n",
      "   200\t    pd._init_image_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n",
      "   201\t    pd._init_image_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n",
      "   202\t    video_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n",
      "   203\t    fps = int(cap.get(cv2.CAP_PROP_FPS))\r\n",
      "   204\t    if tqdm:\r\n",
      "   205\t        pbar = tqdm(total=int(video_len - fps + 1))\r\n",
      "   206\t\r\n",
      "   207\t    out_video = cv2.VideoWriter(\r\n",
      "   208\t        os.path.join(args.output_path, \"output_video.mp4\"),\r\n",
      "   209\t        cv2.VideoWriter_fourcc(*\"avc1\"),\r\n",
      "   210\t        fps,\r\n",
      "   211\t        (pd._init_image_w, pd._init_image_h),\r\n",
      "   212\t        True,\r\n",
      "   213\t    )\r\n",
      "   214\t\r\n",
      "   215\t    counter = 0\r\n",
      "   216\t    start_inference_time = time.time()\r\n",
      "   217\t\r\n",
      "   218\t    try:\r\n",
      "   219\t        while cap.isOpened():\r\n",
      "   220\t            ret, frame = cap.read()\r\n",
      "   221\t            if not ret:\r\n",
      "   222\t                break\r\n",
      "   223\t            counter += 1\r\n",
      "   224\t\r\n",
      "   225\t            if tqdm:\r\n",
      "   226\t                pbar.update(1)\r\n",
      "   227\t\r\n",
      "   228\t            predict_start_time = time.time()\r\n",
      "   229\t            coords, image = pd.predict(frame)\r\n",
      "   230\t            total_inference_time_taken = time.time() - predict_start_time\r\n",
      "   231\t            message = f\"Inference time: {total_inference_time_taken*1000:.2f}ms\"\r\n",
      "   232\t            cv2.putText(\r\n",
      "   233\t                image,\r\n",
      "   234\t                message,\r\n",
      "   235\t                (15, pd._init_image_h - 50),\r\n",
      "   236\t                cv2.FONT_HERSHEY_COMPLEX,\r\n",
      "   237\t                0.75,\r\n",
      "   238\t                (255, 255, 255),\r\n",
      "   239\t                1,\r\n",
      "   240\t            )\r\n",
      "   241\t            num_people = queue.check_coords(coords)\r\n",
      "   242\t\r\n",
      "   243\t            if tqdm:\r\n",
      "   244\t                tqdm.write(f\"Total People in frame = {len(coords)}\")\r\n",
      "   245\t                tqdm.write(f\"Number of people in queue = {num_people}\")\r\n",
      "   246\t            else:\r\n",
      "   247\t                print(f\"Total People in frame = {len(coords)}\")\r\n",
      "   248\t                print(f\"Number of people in queue = {num_people}\")\r\n",
      "   249\t\r\n",
      "   250\t            out_text = \"\"\r\n",
      "   251\t            y_pixel = 25\r\n",
      "   252\t\r\n",
      "   253\t            for k, v in num_people.items():\r\n",
      "   254\t                out_text += f\"No. of People in Queue {k} is {v} \"\r\n",
      "   255\t                cv2.putText(\r\n",
      "   256\t                    image,\r\n",
      "   257\t                    out_text,\r\n",
      "   258\t                    (15, y_pixel),\r\n",
      "   259\t                    cv2.FONT_HERSHEY_COMPLEX,\r\n",
      "   260\t                    1,\r\n",
      "   261\t                    (0, 255, 0),\r\n",
      "   262\t                    2,\r\n",
      "   263\t                )\r\n",
      "   264\t                if v >= int(args.max_people):\r\n",
      "   265\t                    out_text += \" Queue full; Please move to next Queue!\"\r\n",
      "   266\t                    cv2.putText(\r\n",
      "   267\t                        image,\r\n",
      "   268\t                        out_text,\r\n",
      "   269\t                        (15, y_pixel),\r\n",
      "   270\t                        cv2.FONT_HERSHEY_COMPLEX,\r\n",
      "   271\t                        1,\r\n",
      "   272\t                        (0, 0, 255),\r\n",
      "   273\t                        2,\r\n",
      "   274\t                    )\r\n",
      "   275\t                out_text = \"\"\r\n",
      "   276\t                y_pixel += 40\r\n",
      "   277\t\r\n",
      "   278\t            # print total_inference_time_taken\r\n",
      "   279\t            if args.debug:\r\n",
      "   280\t                cv2.imshow(\"Frame\", image)\r\n",
      "   281\t            else:\r\n",
      "   282\t                out_video.write(image)\r\n",
      "   283\t\r\n",
      "   284\t            key = cv2.waitKey(1) & 0xFF\r\n",
      "   285\t            # if the `q` key was pressed, break from the loop\r\n",
      "   286\t            if key == ord(\"q\"):\r\n",
      "   287\t                break\r\n",
      "   288\t\r\n",
      "   289\t        total_time = time.time() - start_inference_time\r\n",
      "   290\t        total_inference_time = round(total_time, 1)\r\n",
      "   291\t        fps = counter / total_inference_time\r\n",
      "   292\t        print(f\"Total time it took to run Inference: {total_inference_time}s\")\r\n",
      "   293\t        print(f\"Frames/Second: {fps}\")\r\n",
      "   294\t\r\n",
      "   295\t        with open(os.path.join(args.output_path, \"stats.txt\"), \"w\") as f:\r\n",
      "   296\t            f.write(str(total_inference_time) + \"\\n\")\r\n",
      "   297\t            f.write(str(fps) + \"\\n\")\r\n",
      "   298\t            f.write(str(total_model_load_time) + \"\\n\")\r\n",
      "   299\t\r\n",
      "   300\t        if tqdm:\r\n",
      "   301\t            pbar.close()\r\n",
      "   302\t        cap.release()\r\n",
      "   303\t        cv2.destroyAllWindows()\r\n",
      "   304\t    except Exception as e:\r\n",
      "   305\t        logger.exception(f\"Could not run Inference: {str(e)}\")\r\n",
      "   306\t\r\n",
      "   307\t\r\n",
      "   308\tif __name__ == \"__main__\":\r\n",
      "   309\t    parser = argparse.ArgumentParser()\r\n",
      "   310\t    parser.add_argument(\r\n",
      "   311\t        \"--model\",\r\n",
      "   312\t        required=True,\r\n",
      "   313\t        help=(\r\n",
      "   314\t            \"The file path of the pre-trained IR model, which has been pre-processed \"\r\n",
      "   315\t            \"using the model optimizer. There is automated support built in this \"\r\n",
      "   316\t            \"argument to support both FP32 and FP16 models targeting different hardware.\"\r\n",
      "   317\t        ),\r\n",
      "   318\t    )\r\n",
      "   319\t    parser.add_argument(\r\n",
      "   320\t        \"--device\",\r\n",
      "   321\t        default=\"CPU\",\r\n",
      "   322\t        help=(\r\n",
      "   323\t            \"The type of hardware you want to load the model on \"\r\n",
      "   324\t            \"(CPU, GPU, MYRIAD, HETERO:FPGA,CPU): [default: CPU]\"\r\n",
      "   325\t        ),\r\n",
      "   326\t    )\r\n",
      "   327\t    parser.add_argument(\"--video\", default=None, help=\"The file path of the input video.\")\r\n",
      "   328\t    parser.add_argument(\r\n",
      "   329\t        \"--output_path\",\r\n",
      "   330\t        default=\"/results\",\r\n",
      "   331\t        help=(\r\n",
      "   332\t            \"The location where the output stats and video file with inference needs \"\r\n",
      "   333\t            \"to be stored (results/[device]).\"\r\n",
      "   334\t        ),\r\n",
      "   335\t    )\r\n",
      "   336\t    parser.add_argument(\r\n",
      "   337\t        \"--max_people\",\r\n",
      "   338\t        default=2,\r\n",
      "   339\t        help=(\r\n",
      "   340\t            \"The max number of people in queue before directing a person to \"\r\n",
      "   341\t            \"another queue.\"\r\n",
      "   342\t        ),\r\n",
      "   343\t    )\r\n",
      "   344\t    parser.add_argument(\r\n",
      "   345\t        \"--threshold\",\r\n",
      "   346\t        default=0.60,\r\n",
      "   347\t        help=(\r\n",
      "   348\t            \"The probability threshold value for the person detection. \"\r\n",
      "   349\t            \"Optional arg; default value is 0.60.\"\r\n",
      "   350\t        ),\r\n",
      "   351\t    )\r\n",
      "   352\t    parser.add_argument(\"--queue_param\", default=None)\r\n",
      "   353\t    parser.add_argument(\r\n",
      "   354\t        \"--debug\", action=\"store_true\", help=\"Show output on screen [debugging].\",\r\n",
      "   355\t    )\r\n",
      "   356\t\r\n",
      "   357\t    args = parser.parse_args()\r\n",
      "   358\t\r\n",
      "   359\t    main(args)\r\n"
     ]
    }
   ],
   "source": [
    "!cat -n person_detect.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
